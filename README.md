# AI ì „ê³µíŠ¹í™”êµìœ¡ í”„ë¡œê·¸ë¨
**ì €ì ìŠ¤íƒ€ì¼ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ë° ë¬¸ì²´ì— ë”°ë¥¸ ê°ì„± ë¶„ì„ ì°¨ì´**

<br/>

## ğŸ“– í”„ë¡œì íŠ¸ ì†Œê°œ
ì´ í”„ë¡œì íŠ¸ëŠ” íŠ¹ì • ì‘ê°€ì˜ ë¬¸ì²´ë¥¼ í•™ìŠµí•œ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸(GPT-2)ì„ êµ¬ì¶•í•˜ê³ , ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ì„í•˜ì—¬ ë¬¸ì²´ì™€ ê°ì„± í‘œí˜„ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. 


<br/>

* íŠ¹ì • ì‘ê°€ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë¬¸ì²´ë¥¼ ë°˜ì˜í•œ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ êµ¬ì¶•
  
* ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ BLEU ì ìˆ˜ ë° ê°ì„± ë¶„í¬ ë¶„ì„
  
* ë¬¸ì²´ ë³´ì¡´ ì—¬ë¶€ê°€ ê°ì„± ë¶„ì„ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ íƒêµ¬
<br/>

----
### ğŸš€ ì½”ë“œ ìˆ˜í–‰ ê³¼ì •
1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
  * ë°ì´í„° ì¶œì²˜: ë°ì´ì½˜ - ì†Œì„¤ ë¬¸ì¥ ë­‰ì¹˜ ë°ì´í„° ì´ìš© 
  * ë¬¸ì¥ ì–¸ì–´: ì˜ì–´ 


* ì „ì²˜ë¦¬:
  - íŠ¹ìˆ˜ë¬¸ìì™€ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
  - ì‰¼í‘œ, ë§ˆì¹¨í‘œ ë“± ë¬¸ì²´ ë³´ì¡´ì— ì¤‘ìš”í•œ êµ¬ë‘ì  ìœ ì§€
  - ê°ì„± ë¼ë²¨ë§: TextBlobìœ¼ë¡œ Positive, Neutral, Negativeë¡œ ë¶„ë¥˜í•˜ê³  ìˆ«ìë¡œ ë§¤í•‘
python
ì½”ë“œ ë³µì‚¬

  #####    ë¬¸ì²´ ë³´ì¡´ ì „ì²˜ë¦¬ í•¨ìˆ˜
  ```
  def clean_text_preserve_style(text):
      import re
      text = re.sub(r"[^\w\s.,!?']", "", text)  # íŠ¹ìˆ˜ë¬¸ì ìœ ì§€
      text = re.sub(r"\s+", " ", text).strip()  # ê³µë°± ì œê±°
      return text
  ```

  
2. ë¬¸ì²´ í•™ìŠµ ëª¨ë¸(GPT-2)
ëª¨ë¸ êµ¬ì„±:
  - Hugging Faceì˜ GPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì‘ê°€(author=3)ì˜ ë°ì´í„°ë¥¼ í•™ìŠµ
  - í•™ìŠµ ë°ì´í„°(90%), í‰ê°€ ë°ì´í„°(10%)ë¡œ ë¶„í•  í›„ Fine-tuning
  - ëª¨ë¸ í•™ìŠµ:
  - í•™ìŠµ ì†ì‹¤ì´ 0.401ë¡œ ìˆ˜ë ´
  - BLEU ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± í’ˆì§ˆ í‰ê°€
  - í…ìŠ¤íŠ¸ ìƒì„±:
  - í”„ë¡¬í”„íŠ¸ "she"ë¡œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼, í•™ìŠµëœ ë¬¸ì²´ë¥¼ ë°˜ì˜í•œ ë¬¸ì¥ì„ ìƒì„±
     - ì˜ˆ: Generated Text: she should have told you then all that was going on in the house.


    ##### GPT-2 ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
    ```
    text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
    generated_text = text_generator("she", max_length=100, num_return_sequences=1)
    print("Generated Text:", generated_text[0]["generated_text"])
    ```


3. ê°ì„± ë¶„ì„ ëª¨ë¸(DistilBERT)
ëª¨ë¸ êµ¬ì„±:
  * TextBlob ê°ì„± ë¶„ì„ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ DistilBERT ê¸°ë°˜ ê°ì„± ë¶„ì„ ëª¨ë¸ êµ¬ì¶•
  * í•™ìŠµ ë°ì´í„°(80%), ê²€ì¦ ë°ì´í„°(10%), í…ŒìŠ¤íŠ¸ ë°ì´í„°(10%)ë¡œ ë¶„í• 
  * ëª¨ë¸ í•™ìŠµ ë° í‰ê°€:
  * ë¬¸ì²´ ë°˜ì˜ ëª¨ë¸(Accuracy): 44.7%
  * ë¬¸ì²´ ë¯¸ë°˜ì˜ ëª¨ë¸(Accuracy): 44.0%
  * Precision, Recall, F1-scoreë¥¼ í†µí•´ ê°ì„± ë¶„ì„ ì„±ëŠ¥ í‰ê°€


    ```
    # Trainer ì •ì˜ ë° í•™ìŠµ
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    trainer.train()
    ```

4. BLEU ì ìˆ˜ ë° ê°ì„± ë¶„ì„ ê²°ê³¼

   
  BLEU ì ìˆ˜:
* í‰ê·  BLEU Score: 0.015~0.023
* ìµœê³  BLEU Score: 0.9199 (í”„ë¡¬í”„íŠ¸ "she")
* ê°ì„± ë¶„ì„ ë¶„í¬:
* Positive: 2ê±´
* Neutral: 1ê±´
* Negative: 0ê±´

##### BLEU ì ìˆ˜ ê³„ì‚°

```
bleu_score = sentence_bleu([ref[0] for ref in references], candidate, weights=(0.5, 0.5), smoothing_function=smoothing)


print(f"BLEU Score: {bleu_score:.4f}")
```

<br/>


## ğŸ” ì£¼ìš” ì‹¤í—˜ ê²°ê³¼
### ë¬¸ì²´ í•™ìŠµ ëª¨ë¸(GPT-2)
  - í”„ë¡¬í”„íŠ¸	ìƒì„± í…ìŠ¤íŠ¸ (Generated Text)	BLEU Score
  - "she"	she should have told you then all that was going on in the house.	0.9199
  - "The day was bright"	The day was bright. A terrible cloud of fire arose on the horizon.	0.0170
### ê°ì„± ë¶„ì„(DistilBERT)
  - ëª¨ë¸	Accuracy	Precision	Recall	F1-Score
  - ë¬¸ì²´ ë³´ì¡´ ëª¨ë¸	44.7%	43.9%	44.7%	43.5%
  - ë¬¸ì²´ ë¯¸ë°˜ì˜ ëª¨ë¸	44.0%	43.5%	44.0%	43.3%

### ğŸ“ˆ ì‹œê°í™”
1. BLEU ì ìˆ˜ íˆìŠ¤í† ê·¸ë¨: ê° í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ BLEU ì ìˆ˜ ë¶„í¬
2. ê°ì„± ë¶„ì„ ê²°ê³¼ ë°” ì°¨íŠ¸:
Positive, Neutral, Negative ë¶„í¬
3. í›ˆë ¨ ì†ì‹¤ ê·¸ë˜í”„: Epochë³„ Training Loss ê°ì†Œ ì¶”ì´
<br/>

## ğŸ›  í•œê³„ ë° ê°œì„  ë°©í–¥
### í•œê³„
  - BLEU ì ìˆ˜ê°€ ì¼ë¶€ í”„ë¡¬í”„íŠ¸ì—ì„œ ë‚®ê²Œ ì¸¡ì •ë¨
    -   â†’ ì°¸ì¡° ë¬¸ì¥ ë‹¤ì–‘ì„± ë¶€ì¡±
  - ê°ì„± ë¶„ì„ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €ì¡°
    - â†’ ë¼ë²¨ë§ ë° ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ
  - ë¬¸ì²´ì™€ ê°ì„± ê°„ ê´€ê³„ë¥¼ ëª…í™•íˆ ì…ì¦í•˜ì§€ ëª»í•¨
    

### ê°œì„  ë°©í–¥
  - ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í™œìš©:
  - ì—¬ëŸ¬ ì‘ê°€ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ëª¨ë¸ ì¼ë°˜í™”
  - í‰ê°€ ì§€í‘œ ë³´ì™„:
  - BLEU ì™¸ì— ROUGE, BERTScore ë“± ì¶”ê°€ ì§€í‘œ í™œìš©
  - ë¼ë²¨ë§ ê°œì„  ë° ë°ì´í„° ì¦ê°•:
  - ê°ì„± ë¼ë²¨ë§ ê³ ë„í™” ë° ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°
<br/>

##ğŸ“š ì°¸ê³ ë¬¸í—Œ


Radford, A., et al., 2019. "Language Models are Few-Shot Learners," Advances in Neural Information Processing Systems.


Zhang, T., et al., 2021. "Style Example-Guided Text Generation using Generative Adversarial Transformers," Journal of Computational Linguistics.
<br/>


## ğŸ“ ì‹¤í–‰ ë°©ë²•
í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
- pip install datasets transformers textblob nltk


- Google Driveì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬.


- train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰.


- generate.pyë¡œ í…ìŠ¤íŠ¸ ìƒì„± ë° BLEU ì ìˆ˜ ê³„ì‚°.
